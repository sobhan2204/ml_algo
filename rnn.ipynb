{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "And now time to see a recurrent neural network in action. For this example, we are going to do something called sentiment analysis.\n",
    "\n",
    "The formal definition of this term from Wikipedia is as follows:\n",
    "\n",
    "*the process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral.*\n",
    "\n",
    "The example we’ll use here is classifying movie reviews as either postive, negative or neutral.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Review Dataset\n",
    "Well start by loading in the IMDB movie review dataset from keras. This dataset contains 25,000 reviews from IMDB where each one is already preprocessed and has a label as either positive or negative. Each review is encoded by integers that represents how common a word is in the entire dataset. For example, a word encoded by the integer 3 means that it is the 3rd most common word in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "vocab_size = 88584 #his value represents the number of unique words in the IMDB dataset that will be considered for training the model.\n",
    "# any data getting 88584 value is the least occuring data in the code\n",
    "maxlen = 250\n",
    "batch_size = 64\n",
    "\n",
    "(train_data , train_lables) , (test_data , test_lables) = imdb.load_data(num_word = vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n",
      "1,13,377,1045,4,420,7,5311,88579,11,4,11282,137,13,16,581,11,1226,2415,36,69,4453,7,90,1266,11,27,1935,137,112,22825,11,2619,24,17,12,679,46,8,2910,4,2619,1066,21,8,2910,90,39,4,1071,14,16,1429,3032,5468,886,13,303,557,15,6,22,69,77,93,34,4285,44,4,420,21,12,16,93,18,298,248,25478,2886,3845,1745,188,6,1763,1360,766,11,1226,2415,13,5339,180,8,4,719,44139,87980,1315,5,16006,1247,6,3602,13,16,584,61,980,33,4,58,16,64,928,11,4,1232,7,4587,32843,5,7157,1419,4808,1318,51,6,530,22,138,286,12,626,8,6,7042,311,69,12,24,77,93,18,248,12,100,28,188,35,735,4372,42,241,50,9,57,96,8,2372,4,277,37,4,455,9,9,115,828,39,4,311,4125,18026,2235,6,371,3240,2167,29,9,4869,16859,1192,6,2104,34,27,322,5,5158,8,1813,12100,5,10473,34,27,9741,33,157,34,8000,134,3388,1421,29,214,8,235,565,976,10,10,997,8,1277,90,429,32,3986,9,6,33407,256,8,321,471,34,1660,7592,11,31,7,27,5655,354,29,215,985,4,15128,1062,7,1766,13666,4,5243,4697,29,47,33,27,11383,5,752,32,4,12665,34,27,9741,8,7539,15,4,14964,60,47,6,1435,455,4,831,11,2909,13219,14029,560,15,1435,2179,26,6,12603,1010,5682,64,2597,3294,9,1688,8,339,21,27,339,215,30,467,4,4837,4,126,530,2431,2653,12850,299,6,1766,3702,37,2010,12536,5,1068,8,339,4,18285,11,68,2682,12,9,4,86,58,11,1766,479,15,6,9551,9,343,8,1703,6,7486,7,6,1435,455,131,23,4,1888,5,29,47,285,8,1585,48,27,3866,9,93,1071,10,10,3845,1745,9,2105,917,73,398,5,4,228,5,802,64,763,8,4,1074,5,756,7,4,22,321,1425,39,6,2357,9673,28519,25897,5,6,147,281,7,8153,4740,97,3845,1745,6,22,290,319,14,16,99,52,8,30,93,18,248,"
     ]
    }
   ],
   "source": [
    "print(len(train_data[0]))\n",
    "p = list(train_data[i])\n",
    "for i in p:\n",
    " print(i , end=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Preprocessing\n",
    "If we have a look at some of our loaded in reviews, we'll notice that they are different lengths. This is an issue. We cannot pass different length data into our neural network. Therefore, we must make each review the same length. To do this we will follow the procedure below:\n",
    "- if the review is greater than 250 words then trim off the extra words\n",
    "- if the review is less than 250 words add the necessary amount of 0's to make it equal to 250."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_data[1]))\n",
    "print(type(test_data[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sequence.pad_sequences(train_data, maxlen)\n",
    "test_data = sequence.pad_sequences(test_data , maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SOBHAN\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_11\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_11\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,834,688</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_11 (\u001b[38;5;33mEmbedding\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │     \u001b[38;5;34m2,834,688\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_11 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,843,041</span> (10.85 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,843,041\u001b[0m (10.85 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,843,041</span> (10.85 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,843,041\u001b[0m (10.85 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=32, input_length=maxlen),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Build the model by calling it with an input shape without this there will be no parameter for the model to be trained on\n",
    "model.build(input_shape=(None, maxlen))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 93ms/step - accuracy: 0.6933 - loss: 0.5518 - val_accuracy: 0.8514 - val_loss: 0.3541\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 118ms/step - accuracy: 0.9282 - loss: 0.1998 - val_accuracy: 0.8686 - val_loss: 0.3192\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 119ms/step - accuracy: 0.9676 - loss: 0.0985 - val_accuracy: 0.8616 - val_loss: 0.3557\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 113ms/step - accuracy: 0.9809 - loss: 0.0625 - val_accuracy: 0.8398 - val_loss: 0.4277\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 109ms/step - accuracy: 0.9804 - loss: 0.0585 - val_accuracy: 0.8646 - val_loss: 0.4520\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 111ms/step - accuracy: 0.9934 - loss: 0.0262 - val_accuracy: 0.8436 - val_loss: 0.6167\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 110ms/step - accuracy: 0.9848 - loss: 0.0441 - val_accuracy: 0.8518 - val_loss: 0.6803\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 107ms/step - accuracy: 0.9960 - loss: 0.0157 - val_accuracy: 0.8566 - val_loss: 0.6655\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 112ms/step - accuracy: 0.9919 - loss: 0.0259 - val_accuracy: 0.8596 - val_loss: 0.6034\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 98ms/step - accuracy: 0.9941 - loss: 0.0187 - val_accuracy: 0.8584 - val_loss: 0.6568\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data , train_lables , epochs=10 , validation_split=0.2) # we'ar using 20% of data as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.8320 - loss: 0.7528\n",
      "[0.7459179759025574, 0.8336799740791321]\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(test_data,test_lables)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "Now let’s use our network to make predictions on our own reviews. \n",
    "\n",
    "Since our reviews are encoded well need to convert any review that we write into that form so the network can understand it. To do that well load the encodings from the dataset and use them to encode our own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "# Load the IMDB word index\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "def encode_text(text):\n",
    "    # Convert the text to a sequence of words\n",
    "    tokens = text_to_word_sequence(text)\n",
    "    # Encode the tokens to their corresponding indices, using 0 for unknown words\n",
    "    tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
    "    # Pad the sequences to a fixed length\n",
    "    return sequence.pad_sequences([tokens], maxlen=250)[0]  # Adjust maxlen as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   1  17  13 477]\n"
     ]
    }
   ],
   "source": [
    "text = \"The movie was amazing\"\n",
    "encoded = encode_text(text)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the movie was amazing\n"
     ]
    }
   ],
   "source": [
    "# Assuming word_index is already defined and encoded is the output from encode_text\n",
    "reverse_index_index = {value: key for (key, value) in word_index.items()}\n",
    "\n",
    "def decode_integer(integers):\n",
    "    text = \"\"  # YE EMPTY STRING HAI JISME HAM KUCH ADD KARENGE BAAD ME \n",
    "    for num in integers:\n",
    "        # HAM SIRF USSI KO ADD KARENGE JO 0 NA HO KYUKI 0 KA MATLAB WAHA KUCH NAHI HAI\n",
    "        if num != 0:  # Skip padding\n",
    "            text += reverse_index_index.get(num, '') + \" \"  # Use get to avoid KeyError AND ISME TEXT STRING ME WORDS AAD KARENGE WITH NUM INDEX AUR FIR SPACE\n",
    "\n",
    "    return text[:-1] # \n",
    "\n",
    "# Example usage\n",
    "decoded_text = decode_integer(encoded)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOW IT'S TIME TO MAKE PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "[0.8886344]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def predict(text):\n",
    "    encodedtext = encode_text(text) # we encode the text\n",
    "    pred = np.zeros((1,250)) # it is a numpy array of shape 1X250 with every single number as 0\n",
    "    pred[0] = encodedtext  # the first letter of numpy array is the text we want\n",
    "    result = model.predict(pred) \n",
    "    print(result[0])\n",
    "\n",
    "positive_predict =  input(\"Write the sentence and see if it;s positive or negetive : \")\n",
    "print(predict(positive_predict)) # HIGGER THE NUMBER THE MORE POSITIVE THE SENTENCE IS :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
